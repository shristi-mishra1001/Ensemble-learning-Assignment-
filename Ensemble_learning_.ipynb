{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "vokDqiuqKyzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer:  \n",
        "\n",
        "Ensemble Learning is a machine learning technique in which multiple individual models, called base learners, are combined to build a single, more powerful predictive model. Instead of relying on one model, ensemble learning aggregates the predictions of several models to improve overall performance.\n",
        "\n",
        "\n",
        "The key idea behind ensemble learning is that a group of diverse models can correct each other’s errors. By combining their predictions (using methods like voting or averaging), ensemble models achieve higher accuracy, better generalization, and reduced overfitting compared to individual models."
      ],
      "metadata": {
        "id": "4PRnzimQK3ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer: Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they differ in how models are trained and combined.\n",
        "\n",
        "Bagging trains multiple models independently on different bootstrap samples drawn randomly from the training dataset. All models are given equal importance, and their predictions are combined using averaging (for regression) or majority voting (for classification). Bagging mainly helps in reducing variance and overfitting.\n",
        "\n",
        "Boosting, on the other hand, trains models sequentially. Each new model focuses more on the data points that were misclassified by previous models. Misclassified samples are given higher importance, and final predictions are made using a weighted combination of all models. Boosting mainly helps in reducing bias and improving accuracy.\n",
        "\n",
        "\n",
        "| Aspect           | Bagging                  | Boosting           |\n",
        "| ---------------- | ------------------------ | ------------------ |\n",
        "| Training style   | Independent              | Sequential         |\n",
        "| Data sampling    | Random bootstrap samples | Reweighted samples |\n",
        "| Model importance | Equal                    | Weighted           |\n",
        "| Main goal        | Reduce variance          | Reduce bias        |\n",
        "| Example          | Random Forest            | AdaBoost, XGBoost  |\n"
      ],
      "metadata": {
        "id": "0cpjt3mVLkeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap sampling is a resampling technique in which multiple training datasets are created by randomly sampling data points from the original dataset with replacement. As a result, some data points may appear multiple times in a sample, while others may not appear at all.\n",
        "\n",
        "In bagging methods like Random Forest, bootstrap sampling is used to train each decision tree on a different subset of the data. This introduces diversity among the trees, reduces correlation between them, and helps in lowering variance. By combining predictions from many independently trained trees, Random Forest achieves better accuracy and robustness compared to a single decision tree."
      ],
      "metadata": {
        "id": "a2cDQACgL4M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        " Out-of-Bag (OOB) samples are the data points that are not selected during bootstrap sampling when training individual models in bagging-based ensemble methods such as Random Forest. On average, about one-third of the original data is left out of each bootstrap sample and becomes OOB data for that model.\n",
        "\n",
        "The OOB score is used as an internal validation method to evaluate the performance of ensemble models without using a separate validation set. Each data point is predicted using only the models for which it was an OOB sample, and the aggregated predictions are compared with the true labels. The resulting OOB score provides an unbiased estimate of the model’s generalization performance."
      ],
      "metadata": {
        "id": "k-o1NBRWMJau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "Answer:\n",
        "\n",
        " Feature importance analysis helps identify which input features contribute most to a model’s predictions. The way feature importance is calculated and interpreted differs between a single Decision Tree and a Random Forest.\n",
        "\n",
        "In a single Decision Tree, feature importance is based on how much a feature reduces impurity (such as Gini Impurity or Entropy) at each split. Since the tree is built on the entire dataset, the importance values can be unstable and sensitive to small changes in the data.\n",
        "\n",
        "In a Random Forest, feature importance is calculated by averaging the impurity reduction across all trees in the forest. Because Random Forest uses multiple trees trained on different bootstrap samples and random feature subsets, the resulting feature importance scores are more stable, reliable, and less prone to overfitting.\n",
        "\n",
        "\n",
        "| Aspect                    | Decision Tree           | Random Forest              |\n",
        "| ------------------------- | ----------------------- | -------------------------- |\n",
        "| Number of models          | Single tree             | Multiple trees             |\n",
        "| Stability                 | Low (sensitive to data) | High (averaged over trees) |\n",
        "| Overfitting               | More likely             | Less likely                |\n",
        "| Reliability of importance | Lower                   | Higher                     |\n",
        "| Generalization            | Weaker                  | Stronger                   |\n"
      ],
      "metadata": {
        "id": "h33CUKotMeqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "yKzQ36IhM1sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and select top 5 features\n",
        "top_5_features = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ").head(5)\n",
        "\n",
        "# Print top 5 important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l0-5L-ZNGl-",
        "outputId": "4f591af8-0f77-4aee-b5e3-47f58f927c20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:->"
      ],
      "metadata": {
        "id": "-ZUP5QM_Ncc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Train Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hjdwlkuNtwO",
        "outputId": "955ad5d0-0608-45fd-9e95-919d89f14171"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "AhKS9d9QNxgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV (cv reduced to avoid errors)\n",
        "grid = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Prediction\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWvP4DhrO1OW",
        "outputId": "434299dd-8224-444a-bce9-5f6556610638"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 50}\n",
            "Final Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "o9QSGYITPJXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODdC_LJqQHGG",
        "outputId": "2bbbbae3-14fb-4049-9622-bebcfffeb67b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0negZocQOfV",
        "outputId": "70478428-beb9-4d89-d563-7ea13a98f143"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25592438609899626\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer -> 1. **Choosing between Bagging or Boosting**\n",
        "\n",
        "To predict loan default, I would first analyze the data complexity and error patterns.\n",
        "\n",
        "Bagging is useful when the base model has high variance and tends to overfit, such as decision trees.\n",
        "\n",
        "Boosting is preferred when the goal is to improve accuracy by focusing on difficult and misclassified cases.\n",
        "\n",
        "Since loan default prediction is a high-risk, imbalanced problem where misclassifying defaulters is costly, Boosting would be the preferred choice because it reduces bias and improves predictive performance.\n",
        "\n",
        "**2. Handling Overfitting**\n",
        "\n",
        "Overfitting can be controlled by:\n",
        "\n",
        "Limiting tree depth and number of estimators\n",
        "\n",
        "Using regularization parameters (learning rate, subsampling)\n",
        "\n",
        "Applying early stopping (in boosting methods)\n",
        "\n",
        "Validating model performance using cross-validation\n",
        "\n",
        "These techniques ensure that the model generalizes well to unseen customer data\n",
        "\n",
        "**3. Selecting Base Models**\n",
        "\n",
        "Decision Trees are chosen as base models because:\n",
        "\n",
        "They capture non-linear relationships in financial data\n",
        "\n",
        "They handle feature interactions effectively\n",
        "\n",
        "They are easy to interpret, which is important in financial institutions\n",
        "\n",
        "Simple decision trees act as weak learners that can be combined effectively in ensemble methods.\n",
        "\n",
        "**4. Evaluating Performance using Cross-Validation**\n",
        "\n",
        "Cross-validation is used to assess model stability and robustness.\n",
        "\n",
        "K-fold cross-validation ensures that the model performs consistently across different subsets of data\n",
        "\n",
        "Evaluation metrics such as ROC-AUC, Recall, and Precision are used instead of accuracy to handle class imbalance\n",
        "\n",
        "This provides a reliable estimate of real-world performance.\n",
        "\n",
        "**5. Business Justification of Ensemble Learning**\n",
        "\n",
        "Ensemble learning improves decision-making by:\n",
        "\n",
        "Increasing prediction accuracy for loan default\n",
        "\n",
        "Reducing financial risk by correctly identifying high-risk customers\n",
        "\n",
        "Supporting fair and data-driven loan approval decisions\n",
        "\n",
        "Enhancing trust and compliance through stable and reliable predictions\n",
        "\n",
        "By combining multiple models, ensemble techniques deliver more robust and reliable results than single models in real-world financial applications.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oRlZgopxQ1Wt"
      }
    }
  ]
}